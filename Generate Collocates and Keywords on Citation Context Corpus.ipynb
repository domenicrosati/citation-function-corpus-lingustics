{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "orange-parish",
   "metadata": {},
   "source": [
    "# Generate Collocates and Keywords on Citation Context Corpus\n",
    "\n",
    "This notebook illustrates how scite citation contexts corpus can be used to generate collocates and keywords by citation function and their basic corpus statistics.\n",
    "\n",
    "This notebook uses a sample citation context set but the same process is used on the larger corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "inappropriate-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pursuant-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are required if not already downloaded\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "common-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "def clean_tokenize_text(text):\n",
    "    tokenized = nltk.word_tokenize(sentence)\n",
    "    words = [word for word in tokenized if (\n",
    "        word.isalpha() and\n",
    "        word not in [\n",
    "        'cite', 'sici', 'lt', 'gt',\n",
    "        ] # remove citation extraction remenants\n",
    "    )]\n",
    "    return words\n",
    "\n",
    "def keywords(tokens):\n",
    "    keywords_count = {\n",
    "        'VERB': {},\n",
    "        'NOUN': {},\n",
    "    }\n",
    "    tags = nltk.pos_tag(tokens, tagset='universal')\n",
    "    for tag in tags:\n",
    "        if tag[0] in stopword_list:\n",
    "            continue\n",
    "       \n",
    "        if tag[1] not in ['VERB', 'NOUN']:\n",
    "            continue\n",
    "        if tag[0] not in keywords_count[tag[1]]:\n",
    "            keywords_count[tag[1]][tag[0]] = 1\n",
    "        else:\n",
    "            keywords_count[tag[1]][tag[0]] += 1\n",
    "    return keywords_count\n",
    "\n",
    "def total_tokens_len(tokens):\n",
    "    return len(tokens)\n",
    "    \n",
    "def total_unique_tokens_len(tokens):\n",
    "    return len(set(tokens))\n",
    "\n",
    "def bigrams(tokens, contain_verb=False):\n",
    "    finder = BigramCollocationFinder.from_words(tokens, window_size=5)\n",
    "    return finder.ngram_fd.items()  \n",
    "\n",
    "def trigrams(tokens, contain_verb=False):\n",
    "    finder = TrigramCollocationFinder.from_words(tokens, window_size=5)\n",
    "    return finder.ngram_fd.items()\n",
    "\n",
    "def quadgrams(tokens, contain_verb=False):\n",
    "    finder = QuadgramCollocationFinder.from_words(tokens, window_size=5)\n",
    "    return finder.ngram_fd.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "close-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./citations_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ready-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = []\n",
    "supporting_tokens = []\n",
    "disputing_tokens = []\n",
    "mentioning_tokens = []\n",
    "for i, row in df.iterrows():\n",
    "    sentence = row['text']\n",
    "    words = clean_tokenize_text(sentence)\n",
    "    total_tokens.extend(words)\n",
    "    if row['type'] == 'supporting':\n",
    "        supporting_tokens.extend(words)\n",
    "    if row['type'] == 'contradicting':\n",
    "        disputing_tokens.extend(words)\n",
    "    if row['type'] == 'mentioning':\n",
    "        mentioning_tokens.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling citation data\n",
      "For supporting citations\n",
      "Total tokens are 257221\n",
      "Total unique tokens are 16462\n",
      "Saving metadata for supporting\n",
      "Saving Verb and Noun Freq\n",
      "Saving Bigrams\n",
      "Saving Trigrams\n",
      "Saving Quadgrams\n",
      "For disputing citations\n",
      "Total tokens are 34640\n",
      "Total unique tokens are 4489\n",
      "Saving metadata for disputing\n",
      "Saving Verb and Noun Freq\n",
      "Saving Bigrams\n",
      "Saving Trigrams\n",
      "Saving Quadgrams\n",
      "For mentioning citations\n",
      "Total tokens are 6326855\n",
      "Total unique tokens are 108675\n",
      "Saving metadata for mentioning\n",
      "Saving Verb and Noun Freq\n",
      "Saving Bigrams\n",
      "Saving Trigrams\n"
     ]
    }
   ],
   "source": [
    "citation_functions = {\n",
    "#     'all': total_tokens,\n",
    "    'supporting': supporting_tokens,\n",
    "    'disputing': disputing_tokens,\n",
    "    'mentioning': mentioning_tokens\n",
    "}\n",
    "\n",
    "print('Compiling citation data')\n",
    "for k, v in citation_functions.items():\n",
    "    print(f'For {k} citations')\n",
    "    print(f'Total tokens are {total_tokens_len(v)}')\n",
    "    print(f'Total unique tokens are {total_unique_tokens_len(v)}')\n",
    "    print(f'Saving metadata for {k}')\n",
    "    pd.DataFrame([\n",
    "        [total_tokens_len(v), total_unique_tokens_len(v)]\n",
    "    ] , columns =['total_tokens', 'total_unique_tokens']).to_csv(f'{k}_metadata.csv')\n",
    "    \n",
    "    print('Saving Verb and Noun Freq')\n",
    "    words = keywords(v)\n",
    "    pd.DataFrame(words['VERB'].items(), columns =['verbs', 'freq']).to_csv(f'{k}_verbs.csv')\n",
    "    pd.DataFrame(words['NOUN'].items(), columns =['nouns', 'freq']).to_csv(f'{k}_nouns.csv')\n",
    "    \n",
    "    print('Saving Bigrams')\n",
    "    bg = bigrams(v)\n",
    "    pd.DataFrame(bg, columns =['bigrams', 'freq']).to_csv(f'{k}_bigrams.csv')\n",
    "\n",
    "    print('Saving Trigrams')\n",
    "    tg = trigrams(v)\n",
    "    pd.DataFrame(tg, columns =['trigrams', 'freq']).to_csv(f'{k}_trigrams.csv')\n",
    "    \n",
    "    print('Saving Quadgrams')\n",
    "    qg = quadgrams(v)\n",
    "    pd.DataFrame(qg, columns =['quadgrams', 'freq']).to_csv(f'{k}_quadgrams.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-antenna",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
